---
title:  "AI has broken digital forensics"
description: "Artifical Intelligence and generative AI have made it so that it's impossible to trust digital media"
date:   2025-07-19 23:27:00 -0400
categories: [AI]
tags: [ai,digital forensics,data integrity]
---
I've been talking with peers about this today, and decided to make a blog post about this. I feel like we've reached the point with generative AI that it's no longer possible for the general public to differentiate between real and fake videos.

I know there's a large body of tech users who are excited about this, and see no issues with AI, but I have my own issues with it.

Being an information security engineer, I feel the following guard rails need to be put in place before things get too out of hand.

## AI Video Identification is Needed

In order to be able to continue to trust digital media, we really need guardrails put in place on technologies like __*Veo 3*__ so that data integrity can be maintained.

[Veo 3](https://deepmind.google/models/veo/)

Whether that be encoding watermarks or signatures in the videos being created, so that sites (and courts of law) can identify that it was AI generated, or the latter where cameras begin to include signatures to prove it's not manipulated.

We are going to reach a point where digitial forensics such as photos, videos, and voice can no longer be trusted.

I have also spoken with friends who have experienced cat fishing where scammers are leveraging these technologies to make more conviencing fake profiles and video verificiation content.

## Currently No Ethical Governance Body and No Legislation around AI

It's going to take decades for the courts to legislate artificial intelligence usage, and by then it's going to be too little too late.

There needs to be accountability for those that are creating malicious content with generative AI, and I would think there should also be guardrails or an opt-out process to allow individuals to have control over their likeness.

Defemation of character, and impersonating others with malicious intent should have consequences.

## AI doesn't provide kickbacks to the original creators

Historically, The internet has been a great way for individuals to share knowledge and content to the world. However, with AI, these models consume the publicly accessible data and don't offer anything in return to the original content creator. At a minimum, there should be a kickback to the source of the data that they used the information from.


## AI doesn't support Data Ownership

Similar to the above section, once the public data is collected into those large langugage models, there's not really anything you can do to control your own data. I feel like the same opt-out policies should apply to AI models, where your data can be deleted upon request, since they really are just becoming a data broker at that point.

One nice thing that i've discovered recently is that Cloudflare has begun implementing ways to block AI crawlers from scraping your site's data and I agree with there permission based approach that gives power back to the data owners.

[Cloudflare AI Crawler Changes](https://www.cloudflare.com/press-releases/2025/cloudflare-just-changed-how-ai-crawlers-scrape-the-internet-at-large/)



